{
  "hash": "73985f5d0ac85cff6f2075eda61a14f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Econometrics - Lecture 6\"\nsubtitle: \"Categorical Predictors and Interaction Effects\"\nauthor: \"Logan Kelly Ph.D.\"\ndate: \"January 28, 2025\"\n---\n\n\n\n## Introduction\n\n  - Categorical Predictors (Dummy Variables)\n  - Interaction Effects\n\n## Key Concepts\n\n- **Categorical Predictors (Dummy Variables)**\n  - Representing qualitative variables quantitatively\n  - Conversion of categorical data into binary indicators\n- **Inclusion of Factors in Regression Models**\n  - How R handles categorical data through factors\n  - Automatic creation of dummy variables\n- **Interpretation of Factor Levels**\n  - Understanding coefficients for different categories\n  - Reference category and its role in interpretation\n- **Interaction Effects**\n  - Concept of variables interacting to influence the dependent variable\n  - Identifying and specifying interactions in regression models\n- **Model Specification for Interactions**\n  - Syntax in R for including interaction terms (`x1 * x2` or `x1:x2`)\n- **Interpretation of Interaction Effects**\n  - How the effect of one variable changes based on the level of another variable\n\n## Theoretical Discussion\n\n### Categorical Predictors (Dummy Variables)\n\n- **Inclusion of Factors**\n  - **Factors in R:**\n    - R treats categorical variables as factors, which are internally represented as integers with associated labels\n    - When included in a regression model, R automatically generates dummy variables for each level except the reference category\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample dataset\ndata <- data.frame(\n  Salary = c(50000, 60000, 55000, 65000, 70000, 62000, 58000, 72000),\n  Education_ = factor(c('Bachelors', 'Masters', 'PhD', 'Bachelors', 'Masters', 'PhD', 'Bachelors', 'PhD')),\n  Experience = c(5, 7, 10, 6, 8, 12, 4, 15),\n  Gender_ = factor(c('Male', 'Female', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male'))\n)\n```\n:::\n\n\n\n  - **Dummy Variable Coding:**\n    - Each level of a categorical variable is represented by a binary (0/1) variable\n    - Ensures that the model can estimate the effect of each category relative to the reference\n\n- **Interpretation of Factor Levels**\n  - **Coefficient Interpretation:**\n    - Each dummy variable's coefficient represents the difference in the dependent variable between that category and the reference category\n  - **Example:**\n    - If `Gender` has levels `Male` and `Female` (with `Male` as the reference), the coefficient for `Female` indicates the average change in the dependent variable when the subject is female compared to male\n\nSuppose we want to model the annual salary (`Salary`) of employees based on their years of experience (`Experience`) and gender (`Gender`). Here, `Gender` is a categorical variable with two categories:\n- **Male** (`Gender = 0`)\n- **Female** (`Gender = 1`)\n\nThe regression equation incorporating the dummy variable for gender can be written as:\n\n$$\n\\text{Salary} = \\beta_0 + \\beta_1 \\times \\text{Experience} + \\beta_2 \\times \\text{Gender} + \\epsilon\n$$\n\nWhere:\n- $\\beta_0$ is the intercept (the expected salary for the reference category when all predictors are zero).\n- $\\beta_1$ is the coefficient for `Experience`, representing the change in salary for each additional year of experience.\n- $\\beta_2$ is the coefficient for `Gender`, representing the difference in salary between females and males.\n- $\\epsilon$ is the error term.\n\nExtensive form\n\n$$\ny=\\begin{cases}\\beta_{0} +\\beta_{1} x\\  +\\varepsilon&\\rm{if\\ Gender\\ =\\ 0}\\\\ (\\beta_{0} +\\beta_2 )+\\beta_{1} \\times x+\\varepsilon&\\rm{if\\ Gender\\ =\\ 1}\\end{cases}\n$$\n\n- **Reference Category**\n  - **Choice of Reference:**\n    - The reference category is crucial as it serves as the baseline for comparison\n    - Changing the reference category alters the interpretation of all other coefficients\n  - **Impact on Interpretation:**\n    - Selecting a meaningful reference category enhances the interpretability of the model\n    - Typically, the most common or a neutral category is chosen as the reference\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(Salary ~ Experience + Gender_, data = data)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Salary ~ Experience + Gender_, data = data)\n\nResiduals:\n      1       2       3       4       5       6       7       8 \n-7185.9  -468.7 -8844.7  6688.7  8406.0 -3063.4   907.4  3560.5 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  52591.2     6750.5   7.791 0.000558 ***\nExperience    1125.4      782.1   1.439 0.209708    \nGender_Male  -1032.1     5471.2  -0.189 0.857797    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7326 on 5 degrees of freedom\nMultiple R-squared:  0.3011,\tAdjusted R-squared:  0.02156 \nF-statistic: 1.077 on 2 and 5 DF,  p-value: 0.4083\n```\n\n\n:::\n:::\n\n\n\n### Interaction Effects\n\n- **When & Why**\n  - **Theory Behind Interactions:**\n    - Interaction effects occur when the effect of one predictor variable on the dependent variable depends on the level of another predictor\n    - Captures the combined influence of two or more variables beyond their individual effects\n  - **Use Cases:**\n    - Situations where variables are believed to synergize or antagonize each other\n    - Enhances model flexibility and accuracy in capturing complex relationships\n\n- **Model Specification**\n  - Syntax in R:\n    - `Education * Experience` expands to `Education + Experience + Education:Experience`, including both main effects and their interaction\n    - `Education:Experience` includes only the interaction term without the main effects $$y=\\begin{cases}\\beta_{0} +\\beta_{1} x\\  +\\varepsilon&\\text{if\\ Education = Bachelors}\\\\ \\beta_{0} +(\\beta_{1} +\\beta_{2} ) \\times x+\\varepsilon&\\text{if\\ Education = Masters}\\\\ \\beta_{0} +(\\beta_{1} +\\beta_{3} ) \\times x+\\varepsilon&\\rm{if\\ Education = PhD}\\end{cases}$$\n  - Example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit a model with interaction between Education and Experience\nmodel_interaction <- lm(Salary ~ Education_ * Experience, data = data)\n\n# Summary of the interaction model\nsummary(model_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Salary ~ Education_ * Experience, data = data)\n\nResiduals:\n         1          2          3          4          5          6          7 \n-7.667e+03  7.190e-12 -7.895e+01  3.833e+03 -6.907e-12  1.316e+02  3.833e+03 \n         8 \n-5.263e+01 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(>|t|)\n(Intercept)                   40166.7    23788.8   1.688    0.233\nEducation_Masters            -50166.7    74490.3  -0.673    0.570\nEducation_PhD                -19035.1    33318.9  -0.571    0.625\nExperience                     3500.0     4695.6   0.745    0.534\nEducation_Masters:Experience   6500.0    10499.6   0.619    0.599\nEducation_PhD:Experience       -105.3     5052.7  -0.021    0.985\n\nResidual standard error: 6641 on 2 degrees of freedom\nMultiple R-squared:  0.7703,\tAdjusted R-squared:  0.1962 \nF-statistic: 1.342 on 5 and 2 DF,  p-value: 0.4792\n```\n\n\n:::\n:::\n\n\n**Visualize interaction effect**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(ggplot2)\nggplot(data, aes(x = Experience, y = Salary, color = Education_)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Interaction between Education and Experience on Salary\",\n       x = \"Years of Experience\",\n       y = \"Salary\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Lecture_M01-L06_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n- **Interpretation**\n  - **Coefficient of Interaction Term:**\n    - Represents how the relationship between one predictor and the dependent variable changes with the level of the other predictor\n  - **Visualization:**\n    - Interaction effects can be visualized using interaction plots to illustrate how slopes differ across levels of interacting variables\n  - **Practical Implications:**\n    - Provides deeper insights into the dynamics between variables, aiding in more informed decision-making\n\n## Case Study\n\n- **Objective:**\n  - Analyze the impact of the number of gears and weight on a car's fuel efficiency (`mpg`), considering transmission type differences.\n\n- **Dataset Overview:**\n  - **Variables:**\n    - **mpg:** Dependent variable (miles per gallon)\n    - **gear:** Categorical predictor (number of forward gears: 3, 4, 5)\n    - **wt:** Independent variable (weight of the car in 1000 lbs)\n    - **am:** Categorical predictor (transmission type: 0 = automatic, 1 = manual)\n\n- **Methodology:**\n  - **Step 1:** Encode categorical variables (`gear` and `am`) as factors in R\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the mtcars dataset\ndata <- mtcars\n\n# Encode 'gear' and 'am' as factors\ndata$gear <- factor(data$gear, levels = c(3, 4, 5),\n                    labels = c(\"3 Gears\", \"4 Gears\", \"5 Gears\"))\ndata$am <- factor(data$am, levels = c(0, 1),\n                 labels = c(\"Automatic\", \"Manual\"))\n```\n:::\n\n\n\n  - **Step 2:** Fit a multiple regression model including `gear` and `am` as categorical predictors\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit multiple regression model with categorical predictors\nmodel_main <- lm(mpg ~ gear + wt + am, data = data)\nsummary(model_main)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ gear + wt + am, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5798 -2.4056 -0.3692  1.8198  5.7713 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.0955     3.1862  11.015 1.72e-11 ***\ngear4 Gears   2.0769     1.7343   1.198    0.242    \ngear5 Gears  -1.0615     2.3845  -0.445    0.660    \nwt           -4.8782     0.7945  -6.140 1.46e-06 ***\namManual      0.1883     1.9942   0.094    0.925    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.968 on 27 degrees of freedom\nMultiple R-squared:  0.7888,\tAdjusted R-squared:  0.7575 \nF-statistic:  25.2 on 4 and 27 DF,  p-value: 8.931e-09\n```\n\n\n:::\n:::\n\n\n  \n  - **Step 3:** Introduce an interaction term between `gear` and `wt` to explore combined effects\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit regression model with interaction between gear and wt\nmodel_interaction <- lm(mpg ~ gear * wt + am, data = data)\nsummary(model_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ gear * wt + am, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9929 -1.7570  0.0941  0.9556  5.3892 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     28.3950     3.0053   9.448 9.95e-10 ***\ngear4 Gears     26.1964     7.0213   3.731 0.000985 ***\ngear5 Gears     19.4213     5.5105   3.524 0.001661 ** \nwt              -3.1569     0.7561  -4.175 0.000315 ***\namManual        -5.2535     2.4242  -2.167 0.039955 *  \ngear4 Gears:wt  -6.9918     2.0325  -3.440 0.002051 ** \ngear5 Gears:wt  -4.8895     1.6253  -3.008 0.005919 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.356 on 25 degrees of freedom\nMultiple R-squared:  0.8767,\tAdjusted R-squared:  0.8471 \nF-statistic: 29.63 on 6 and 25 DF,  p-value: 3.325e-10\n```\n\n\n:::\n:::\n\n\n\n  - **Step 4:** F-test on interaction term \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compare models\nanova(model_main, model_interaction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: mpg ~ gear + wt + am\nModel 2: mpg ~ gear * wt + am\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     27 237.87                                \n2     25 138.82  2    99.047 8.9183 0.001193 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n  - **Step 5:** Visualize the results \n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize interaction effect\nggplot(data, aes(x = wt, y = mpg, color = gear)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Interaction between Number of Gears and Weight on MPG\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Lecture_M01-L06_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n  - **Step 6:** Residual Diagnostics\n  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Residuals vs Fitted Values Plot\nggplot(model_interaction, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(method = \"loess\", se = FALSE, color = \"blue\") +\n  labs(title = \"Residuals vs Fitted Values\",\n       x = \"Fitted Values\",\n       y = \"Residuals\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Lecture_M01-L06_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n## Conclusion\n\n  - Takeaway 1: Properly encoding and interpreting categorical variables is essential for accurate regression analysis\n  - Takeaway 2: Interaction effects reveal deeper insights into the relationships between predictors\n  - Takeaway 3: Model specification in R is straightforward, allowing for complex model building with interactions\n",
    "supporting": [
      "Lecture_M01-L06_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}