{
  "hash": "1b7c668ec387d8000fc776fe23676228",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ECON426: Further Inference in Multiple Regression using cps5 Data\"\nformat:\n  html:\n    code-fold: false\n    number-sections: false\n---\n\n\n\n# Introduction\n\nIn this assignment you will extend your cps5 data project by exploring advanced topics in multiple regression inference. You will examine joint hypotheses, test simultaneous restrictions, assess omitted variable bias, evaluate the impact of irrelevant variables, compare model selection criteria, and check for collinearity. Although your final analysis must be performed on the cps5 dataset, the step‐by‐step instructions and example code below use the built‑in mtcars dataset as a model. Carefully study each section and then replicate the analyses using cps5.\n\nThe topics you will cover are:\n\n1. Data Loading and Setup  \n2. Joint Hypotheses and the F‑Statistic  \n3. Testing Simultaneous Hypotheses  \n4. Omitted Variable Bias  \n5. Irrelevant Variables  \n6. Model Selection Criteria  \n7. Collinearity  \n\n# 1. Data Loading and Setup\n\n**Objective:**  \nPrepare your workspace by clearing previous objects, loading the necessary packages, and inspecting your cps5 dataset.\n\n**Instructions for Your cps5 Analysis:**  \n1. Load pacman package.  \n2. Load the required packages (e.g., tidyverse, car, broom).  \n3. Load the cps5 dataset and inspect its structure using `glimpse()`.\n\n**Example (using mtcars as a model):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Load required packages using pacman\nif(!require(\"pacman\")) install.packages(\"pacman\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: pacman\n```\n\n\n:::\n\n```{.r .cell-code}\npacman::p_load(tidyverse, car, broom)\n\n# Step 2: Load and inspect the mtcars dataset (for demonstration)\ndata(mtcars)\nglimpse(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n```\n\n\n:::\n:::\n\n\n\n*Note:* In your final work, replace the mtcars dataset with cps5 (e.g., use `data(\"cps5\")` and `glimpse(cps5)`).\n\n# 2. Joint Hypotheses and the F‑Statistic\n\n**Objective:**  \nTest a joint hypothesis using the F‑statistic by comparing an unrestricted model (with all predictors) against the null hypothesis that two coefficients are zero.\n\n**Instructions for Your cps5 Analysis:**  \n1. **Fit the Unrestricted Model:**  \n   Using the cps5 dataset, fit a regression model that predicts wage using all your chosen numeric predictors (for example, age, educ, exper, faminc, hrswork, and nchild).  \n2. **Specify the Joint Hypothesis:**  \n   Define the null hypothesis that two coefficients (for instance, the effects of faminc and nchild) are both equal to zero.  \n3. **Run the Joint Test:**  \n   Use the `linearHypothesis()` function from the car package to test this joint hypothesis.  \n4. **Interpret the Results:**  \n   Examine the F‑statistic and p‑value. A low p‑value indicates that you should reject the null hypothesis, implying that at least one of the two predictors significantly influences wage.\n\n**Example (using mtcars as a model):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Fit the unrestricted model on mtcars.\nmod_full <- lm(mpg ~ wt + hp + disp, data = mtcars)\n\n# Step 2: Specify the joint hypothesis: test if hp = 0 and disp = 0.\nhypothesis <- c(\"hp = 0\", \"disp = 0\")\n\n# Step 3: Run the joint hypothesis test using linearHypothesis()\nlh_test <- linearHypothesis(mod_full, hypothesis)\nprint(lh_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nhp = 0\ndisp = 0\n\nModel 1: restricted model\nModel 2: mpg ~ wt + hp + disp\n\n  Res.Df    RSS Df Sum of Sq     F   Pr(>F)   \n1     30 278.32                               \n2     28 194.99  2    83.331 5.983 0.006863 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n**Explanation:**  \nIn this example, the regression model on mtcars predicts mpg using wt, hp, and disp. The joint hypothesis test checks whether both hp and disp have no effect (i.e., their coefficients equal zero). The output includes an F‑statistic and a p‑value. A low p‑value suggests that at least one predictor is significant. For your cps5 analysis, you might test whether faminc and nchild jointly have no effect on wage.\n\n# 3. Testing Simultaneous Hypotheses\n\n**Objective:**  \nExamine whether specific linear combinations of regression coefficients are simultaneously equal to zero.\n\n**Instructions for Your cps5 Analysis:**  \n1. **Define the Simultaneous Hypotheses:**  \n   Formulate hypotheses that reflect relationships you want to test (for example, that the difference between two coefficients is zero).  \n2. **Run the Joint Test:**  \n   Use `linearHypothesis()` to test these simultaneous restrictions on your cps5 model.  \n3. **Interpret the Output:**  \n   Focus on the F‑statistic and p‑value to determine if the combined restrictions hold.\n\n**Example (using mtcars as a model):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Define the simultaneous hypotheses.\n# Example: test if the difference between wt and hp is zero,\n# and if the difference between disp and hp is zero.\njoint_hyp <- c(\"wt - hp = 0\", \"disp - hp = 0\")\n\n# Step 2: Test the simultaneous hypotheses on the full mtcars model.\nlh_joint <- linearHypothesis(mod_full, joint_hyp)\nprint(lh_joint)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nLinear hypothesis test:\nwt - hp = 0\n- hp  + disp = 0\n\nModel 1: restricted model\nModel 2: mpg ~ wt + hp + disp\n\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     30 282.87                                \n2     28 194.99  2    87.874 6.3092 0.005471 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n**Explanation:**  \nThis example tests whether the difference between the coefficients for wt and hp is zero, as well as whether the difference between disp and hp is zero. The output helps determine whether these specific relationships hold. For your cps5 analysis, adjust the hypotheses based on your theoretical expectations.\n\n# 4. Omitted Variable Bias\n\n**Objective:**  \nDetermine the effect on estimated coefficients when a relevant variable is omitted from the model.\n\n**Instructions for Your cps5 Analysis:**  \n1. **Fit the Full Model:**  \n   Regress wage on all your chosen predictors (e.g., age, educ, exper, faminc, hrswork, nchild) using cps5.  \n2. **Fit the Restricted Model:**  \n   Omit one key variable (for example, omit educ) from the model.  \n3. **Compare the Results:**  \n   Compare the regression outputs—especially the estimated coefficient(s) of another predictor—to assess the potential bias.\n\n**Example (using mtcars as a model):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Fit the full model: mpg ~ wt + hp + disp\nmod_full_bias <- lm(mpg ~ wt + hp + disp, data = mtcars)\nsummary(mod_full_bias)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + hp + disp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.891 -1.640 -0.172  1.061  5.861 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.105505   2.110815  17.579  < 2e-16 ***\nwt          -3.800891   1.066191  -3.565  0.00133 ** \nhp          -0.031157   0.011436  -2.724  0.01097 *  \ndisp        -0.000937   0.010350  -0.091  0.92851    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.639 on 28 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8083 \nF-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 2: Fit the restricted model: omit hp (i.e., mpg ~ wt + disp)\nmod_omitted <- lm(mpg ~ wt + disp, data = mtcars)\nsummary(mod_omitted)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + disp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4087 -2.3243 -0.7683  1.7721  6.3484 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.96055    2.16454  16.151 4.91e-16 ***\nwt          -3.35082    1.16413  -2.878  0.00743 ** \ndisp        -0.01773    0.00919  -1.929  0.06362 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.917 on 29 degrees of freedom\nMultiple R-squared:  0.7809,\tAdjusted R-squared:  0.7658 \nF-statistic: 51.69 on 2 and 29 DF,  p-value: 2.744e-10\n```\n\n\n:::\n:::\n\n\n\n**Explanation:**  \nThis example shows two models: a full model that includes wt, hp, and disp, and a restricted model that omits hp. Comparing the estimates for wt between these models reveals how omitting a relevant variable (hp) can bias the effect of another predictor. For cps5, you might omit educ to examine the bias on another variable’s coefficient.\n\n# 5. Irrelevant Variables\n\n**Objective:**  \nDetermine how including variables that theoretically should not affect the outcome impacts your regression estimates.\n\n**Instructions for Your cps5 Analysis:**  \n1. **Create Irrelevant Variables:**  \n   Generate one or two artificial variables (using random noise) that are not expected to affect wage.  \n2. **Fit the Base Model:**  \n   Estimate a model using only the relevant predictors on cps5.  \n3. **Fit the Extended Model:**  \n   Estimate an extended model that includes the irrelevant variables.  \n4. **Compare the Outputs:**  \n   Compare the coefficient estimates and standard errors between the two models.\n\n**Example (using mtcars as a model):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Create two artificial irrelevant variables\nset.seed(123)\nmtcars <- mtcars %>%\n  mutate(irrel1 = rnorm(nrow(mtcars)),\n         irrel2 = rnorm(nrow(mtcars)))\n\n# Step 2: Fit the model without irrelevant variables: mpg ~ wt + hp + disp\nmod_no_irrel <- lm(mpg ~ wt + hp + disp, data = mtcars)\nsummary(mod_no_irrel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + hp + disp, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.891 -1.640 -0.172  1.061  5.861 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 37.105505   2.110815  17.579  < 2e-16 ***\nwt          -3.800891   1.066191  -3.565  0.00133 ** \nhp          -0.031157   0.011436  -2.724  0.01097 *  \ndisp        -0.000937   0.010350  -0.091  0.92851    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.639 on 28 degrees of freedom\nMultiple R-squared:  0.8268,\tAdjusted R-squared:  0.8083 \nF-statistic: 44.57 on 3 and 28 DF,  p-value: 8.65e-11\n```\n\n\n:::\n\n```{.r .cell-code}\n# Step 3: Fit the model with irrelevant variables: mpg ~ wt + hp + disp + irrel1 + irrel2\nmod_with_irrel <- lm(mpg ~ wt + hp + disp + irrel1 + irrel2, data = mtcars)\nsummary(mod_with_irrel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mpg ~ wt + hp + disp + irrel1 + irrel2, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3248 -1.6137 -0.5814  1.1538  5.7913 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 36.670729   2.266890  16.177 4.35e-15 ***\nwt          -3.645344   1.128673  -3.230  0.00335 ** \nhp          -0.029688   0.011835  -2.508  0.01870 *  \ndisp        -0.002147   0.010828  -0.198  0.84439    \nirrel1      -0.433175   0.530125  -0.817  0.42129    \nirrel2      -0.164924   0.581845  -0.283  0.77908    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.7 on 26 degrees of freedom\nMultiple R-squared:  0.8317,\tAdjusted R-squared:  0.7993 \nF-statistic: 25.69 on 5 and 26 DF,  p-value: 2.729e-09\n```\n\n\n:::\n:::\n\n\n\n**Explanation:**  \nThe inclusion of irrelevant variables (irrel1 and irrel2) should not substantially change the coefficients of the main predictors but may inflate their standard errors. Compare the models to determine the effect of including these noise variables. In your cps5 analysis, add variables that theory suggests should not influence wage and discuss the impact.\n\n# 6. Model Selection Criteria\n\n**Objective:**  \nCompare multiple models using criteria that balance model fit and parsimony.\n\n**Instructions for Your cps5 Analysis:**  \n1. **Estimate Several Models:**  \n   Build several regression models on cps5 with different combinations of predictors (for example, start with a simple model and gradually add variables like age, educ, exper, faminc, hrswork, and nchild).  \n2. **Extract Model Statistics:**  \n   Use the `glance()` function from the broom package to retrieve R², adjusted R², AIC, and BIC for each model.  \n3. **Compare the Models:**  \n   Compare these statistics to decide which model provides the best balance of fit and simplicity.\n\n**Example (using mtcars as a model):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Estimate alternative models\nmodA <- lm(mpg ~ wt, data = mtcars)\nmodB <- lm(mpg ~ wt + hp, data = mtcars)\nmodC <- lm(mpg ~ wt + hp + disp, data = mtcars)\nmodD <- lm(mpg ~ wt + hp + disp + drat, data = mtcars)\n\n# Step 2: Extract model statistics using glance()\nstatsA <- glance(modA)[, c(\"r.squared\", \"adj.r.squared\", \"AIC\", \"BIC\")]\nstatsB <- glance(modB)[, c(\"r.squared\", \"adj.r.squared\", \"AIC\", \"BIC\")]\nstatsC <- glance(modC)[, c(\"r.squared\", \"adj.r.squared\", \"AIC\", \"BIC\")]\nstatsD <- glance(modD)[, c(\"r.squared\", \"adj.r.squared\", \"AIC\", \"BIC\")]\n\n# Step 3: Print the statistics for comparison\nprint(statsA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.753         0.745  166.  170.\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(statsB)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.827         0.815  157.  163.\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(statsC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.827         0.808  159.  166.\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(statsD)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  r.squared adj.r.squared   AIC   BIC\n      <dbl>         <dbl> <dbl> <dbl>\n1     0.838         0.814  159.  167.\n```\n\n\n:::\n:::\n\n\n\n**Explanation:**  \nAlthough R² generally increases with more predictors, adjusted R², AIC, and BIC penalize unnecessary complexity. These measures help you decide which model best predicts mpg without overfitting. For cps5, build models predicting wage with different combinations of variables and select the best based on these criteria.\n\n# 7. Collinearity\n\n**Objective:**  \nAssess collinearity among predictors by calculating Variance Inflation Factors (VIF).\n\n**Instructions for Your cps5 Analysis:**  \n1. **Fit a Multivariate Model:**  \n   Estimate a model using your chosen predictors on cps5.  \n2. **Calculate VIFs:**  \n   Use the `vif()` function from the car package to compute VIF values for each predictor.  \n3. **Interpret the VIF Values:**  \n   Examine the VIF values—typically, a VIF above 10 indicates problematic collinearity.\n\n**Example (using mtcars as a model):**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Fit a multivariate model: mpg ~ wt + hp + disp + drat\nmod_collinearity <- lm(mpg ~ wt + hp + disp + drat, data = mtcars)\n\n# Step 2: Calculate Variance Inflation Factors (VIF)\nvif_values <- vif(mod_collinearity)\n\n# Step 3: Print the VIF values\nprint(vif_values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      wt       hp     disp     drat \n5.096601 2.894373 8.209402 2.279547 \n```\n\n\n:::\n:::\n\n\n\n**Explanation:**  \nVIF values show how much the variance of an estimated coefficient increases due to multicollinearity. High VIFs (commonly > 10) signal that a predictor is highly correlated with others, which can make coefficient estimates less reliable. For your cps5 analysis, calculate and discuss the VIFs for your selected predictors.\n\n# Conclusion\n\nIn your final report for the cps5 data project, be sure to include the following:\n\n- **Introduction:**  \n  Describe your research questions, the rationale for using cps5, and your overall analytical approach.\n- **Joint Hypotheses and the F‑Statistic:**  \n  Present your joint hypothesis test results (using your cps5 model) and interpret the F‑statistic and p‑value.\n- **Testing Simultaneous Hypotheses:**  \n  Discuss the results of your simultaneous hypothesis tests and their implications.\n- **Omitted Variable Bias:**  \n  Compare the full and restricted cps5 models and explain any observed bias when omitting a key variable.\n- **Irrelevant Variables:**  \n  Report how the inclusion of irrelevant variables affects your model’s coefficients and precision.\n- **Model Selection Criteria:**  \n  Compare multiple cps5 models using R², adjusted R², AIC, and BIC, and justify your model choice.\n- **Collinearity:**  \n  Evaluate collinearity using VIF values and discuss any concerns.\n- **Conclusion:**  \n  Summarize your key findings and provide recommendations for further model refinement.\n\n# Submission Instructions\n\n- **Nicely Format Your Results:** Use Quarto or RMarkdown to clearly present your code and results.\n- **Render to MS Word:** Once your work is complete, render your file to produce an MS Word (.docx) document.\n- **Submission:** Upload the rendered document to the Canvas Dropbox.\n",
    "supporting": [
      "Homework04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}